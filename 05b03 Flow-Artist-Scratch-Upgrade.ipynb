{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5fd9c",
   "metadata": {},
   "source": [
    "### Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def affinity_from_flow(flows, directions_array, flow_strength = 1, sigma=1):\n",
    "  \"\"\"Compute probabilities of transition in the given directions based on the flow. \n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  flows : torch tensor of shape n_points x n_dims\n",
    "      _description_\n",
    "  directions_array : torch tensor of shape n_directions x n_points x n_dims. Assumed to be normalized.\n",
    "      _description_\n",
    "  sigma : int, optional\n",
    "      kernel bandwidth, by default 1\n",
    "  returns (n_points)\n",
    "  \"\"\"\n",
    "  assert len(flows.shape) == 2 # flows should only have one dimension\n",
    "  assert len(directions_array.shape) > 1 and len(directions_array.shape) < 4\n",
    "  n_directions = directions_array.shape[0]\n",
    "  # Normalize directions\n",
    "  length_of_directions = torch.linalg.norm(directions_array,dim=-1)\n",
    "  normed_directions = F.normalize(directions_array,dim=-1)\n",
    "  # and normalize flows # TODO: Perhaps reconsider\n",
    "  flows = F.normalize(flows,dim=-1)\n",
    "\n",
    "  if len(directions_array) == 1: # convert to 2d array if necessary\n",
    "    directions_array = directions_array[:,None] \n",
    "  # compute dot products as matrix multiplication\n",
    "  dot_products = (normed_directions * flows).sum(-1)\n",
    "  # take distance between flow projected onto direction and the direction\n",
    "  distance_from_flow = (torch.linalg.norm(flows,dim=1)**2).repeat(n_directions,1) - dot_products\n",
    "  # take absolute value\n",
    "  distance_from_flow = torch.abs(distance_from_flow)\n",
    "  # print('shape of dff',distance_from_flow.shape)\n",
    "  # add to this the length of each direction\n",
    "  distance_from_flow = flow_strength*distance_from_flow + length_of_directions\n",
    "  # put the points on rows, directions in columns\n",
    "  distance_from_flow = distance_from_flow.T\n",
    "  # take kernel of distances\n",
    "  kernel =  torch.exp(-distance_from_flow/sigma)\n",
    "  # normalize kernel\n",
    "  # kernel /= torch.sum(kernel,axis=1)\n",
    "  return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54703c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch.nn.functional as F\\ndef affinity_from_flow(flows, directions_array, flow_strength = 1, sigma=1):\\n  \"\"\"Compute probabilities of transition in the given directions based on the flow. \\n\\n  Parameters\\n  ----------\\n  flows : torch tensor of shape n_points x n_dims\\n      _description_\\n  directions_array : torch tensor of shape n_directions x n_points x n_dims. Assumed to be normalized.\\n      _description_\\n  sigma : int, optional\\n      kernel bandwidth, by default 1\\n  returns (n_points)\\n  \"\"\"\\n  assert len(flows.shape) == 2 # flows should only have one dimension\\n  assert len(directions_array.shape) > 1 and len(directions_array.shape) < 4\\n  n_directions = directions_array.shape[0]\\n  # Normalize directions\\n  length_of_directions = torch.linalg.norm(directions_array,dim=-1)\\n  normed_directions = F.normalize(directions_array,dim=-1)\\n  # and normalize flows # TODO: Perhaps reconsider\\n  flows = F.normalize(flows,dim=-1)\\n\\n  if len(directions_array) == 1: # convert to 2d array if necessary\\n    directions_array = directions_array[:,None] \\n  # compute dot products as matrix multiplication\\n  dot_products = (normed_directions * flows).sum(-1)\\n  # take distance between flow projected onto direction and the direction\\n  distance_from_flow = (torch.linalg.norm(flows,dim=1)**2).repeat(n_directions,1) - dot_products\\n  # take absolute value\\n  distance_from_flow = torch.abs(distance_from_flow)\\n  # print(\\'shape of dff\\',distance_from_flow.shape)\\n  # add to this the length of each direction\\n  distance_from_flow = flow_strength*distance_from_flow + length_of_directions\\n  # put the points on rows, directions in columns\\n  distance_from_flow = distance_from_flow.T\\n  # take kernel of distances\\n  kernel =  torch.exp(-distance_from_flow/sigma)\\n  # normalize kernel\\n  # kernel /= torch.sum(kernel,axis=1)\\n  return kernel\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch.nn.functional as F\n",
    "def affinity_from_flow(flows, directions_array, flow_strength = 1, sigma=1):\n",
    "  \"\"\"Compute probabilities of transition in the given directions based on the flow. \n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  flows : torch tensor of shape n_points x n_dims\n",
    "      _description_\n",
    "  directions_array : torch tensor of shape n_directions x n_points x n_dims. Assumed to be normalized.\n",
    "      _description_\n",
    "  sigma : int, optional\n",
    "      kernel bandwidth, by default 1\n",
    "  returns (n_points)\n",
    "  \"\"\"\n",
    "  assert len(flows.shape) == 2 # flows should only have one dimension\n",
    "  assert len(directions_array.shape) > 1 and len(directions_array.shape) < 4\n",
    "  n_directions = directions_array.shape[0]\n",
    "  # Normalize directions\n",
    "  length_of_directions = torch.linalg.norm(directions_array,dim=-1)\n",
    "  normed_directions = F.normalize(directions_array,dim=-1)\n",
    "  # and normalize flows # TODO: Perhaps reconsider\n",
    "  flows = F.normalize(flows,dim=-1)\n",
    "\n",
    "  if len(directions_array) == 1: # convert to 2d array if necessary\n",
    "    directions_array = directions_array[:,None] \n",
    "  # compute dot products as matrix multiplication\n",
    "  dot_products = (normed_directions * flows).sum(-1)\n",
    "  # take distance between flow projected onto direction and the direction\n",
    "  distance_from_flow = (torch.linalg.norm(flows,dim=1)**2).repeat(n_directions,1) - dot_products\n",
    "  # take absolute value\n",
    "  distance_from_flow = torch.abs(distance_from_flow)\n",
    "  # print('shape of dff',distance_from_flow.shape)\n",
    "  # add to this the length of each direction\n",
    "  distance_from_flow = flow_strength*distance_from_flow + length_of_directions\n",
    "  # put the points on rows, directions in columns\n",
    "  distance_from_flow = distance_from_flow.T\n",
    "  # take kernel of distances\n",
    "  kernel =  torch.exp(-distance_from_flow/sigma)\n",
    "  # normalize kernel\n",
    "  # kernel /= torch.sum(kernel,axis=1)\n",
    "  return kernel\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b54d03",
   "metadata": {},
   "source": [
    "### Calculate Directions between point i to point j and apply Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affinity_matrix_from_pointset_to_pointset(pointset1, pointset2, flows,n_neighbors=None,sigma=0.5, flow_strength=1):\n",
    "  \"\"\"Compute affinity matrix between the points of pointset1 and pointset2, using the provided flow.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  pointset1 : torch tensor, n1 x d\n",
    "      The first pointset, to calculate affinities *from*\n",
    "  pointset2 : torch tensor, n2 x d\n",
    "      The second pointset, to calculate affinities *to* (from pointset1)\n",
    "  flow : a function that, when called at a point, gives the flow at that point\n",
    "  n_neighbors : number of neighbors to include in affinity computations. All neighbors beyond it are given affinity zero\n",
    "  (currently not implemented)\n",
    "\n",
    "  Returns:\n",
    "  Affinity matrix: torch tensor of shape n1 x n2\n",
    "  \"\"\"\n",
    "  # Calculate the directions from point i in pointset 1 to point j in pointset 2\n",
    "  n1 = pointset1.shape[0]\n",
    "  n2 = pointset2.shape[0]\n",
    "  P2 = pointset2[:,:,None].repeat(1,1,n1)\n",
    "  P1 = pointset1.T.repeat(n2,1,1)\n",
    "  P3 = (P2 - P1)\n",
    "  P3 = P3.transpose(1,2)\n",
    "  # compute affinities from flows and directions\n",
    "  affinities = affinity_from_flow(flows,P3,sigma=sigma,flow_strength=flow_strength)\n",
    "  return affinities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0902f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from tqdm import trange\n",
    "from directed_graphs.utils import diffusion_matrix_from_graph\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DiffusionFlowEmbedder(torch.nn.Module):\n",
    "\tdef __init__(self, X, flows, t = 4, sigma_graph = 0.5, sigma_embedding=0.5, embedding_dimension=2, device=torch.device('cpu'), autoencoder_shape = [100,10], flow_artist_shape = [10,20,10], flow_strength=1):\n",
    "\t\t\"\"\"Flow Embedding with diffusion\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tX : torch tensor n_points x n_dim\n",
    "\t\t\tdata matrix\n",
    "\t\tflows : torch tensor n_points x n_dim\n",
    "\t\t\tThe flow at each point\n",
    "\t\tt : int\n",
    "\t\t\tLoss is computed with the diffusion operator powered to this number\n",
    "\t\tsigma in [0,1]\n",
    "\t\t\tKernel bandwidth in the embedding\n",
    "\t\t\"\"\"\n",
    "\t\t# initialize parameters\n",
    "\t\tsuper(DiffusionFlowEmbedder, self).__init__()\n",
    "\t\tself.X = X\n",
    "\t\tself.ground_truth_flows = flows\n",
    "\t\tself.t = t\n",
    "\t\tself.sigma_embedding = sigma_embedding\n",
    "\t\tself.sigma_graph = sigma_graph\n",
    "\t\tself.nnodes = X.shape[0]\n",
    "\t\tself.data_dimension = X.shape[1]\n",
    "\t\tself.losses = []\n",
    "\t\tself.eps = 0.001\n",
    "\t\tif flow_strength == \"learnable\":\n",
    "\t\t\tself.flow_strength = nn.Parameter(torch.tensor(1.0))\n",
    "\t\telse:\n",
    "\t\t\tself.flow_strength = flow_strength\t\n",
    "\n",
    "\t\tself.embedding_dimension = embedding_dimension\n",
    "\t\t# set device (used for shuffling points around during visualization)\n",
    "\t\tself.device = device\n",
    "\t\t# Compute P^t of the graph, the powered diffusion matrix\n",
    "\t\t# TODO: This can be optimized using landmarks, etc. For now it's straight sparse matrix multiplication\n",
    "\t\t# TODO: Migrate to a specialized function for dataset affinity calculation, with automatic kernel bandwidth selection, and the like\n",
    "\t\tself.P_graph = affinity_matrix_from_pointset_to_pointset(X,X,flows,sigma=sigma_graph)\n",
    "\t\tself.P_graph_t = torch.matrix_power(self.P_graph,self.t)\n",
    "\t\t# Flow field\n",
    "\t\tself.FlowArtist = nn.Sequential(nn.Linear(self.embedding_dimension, flow_artist_shape[0]),\n",
    "\t\t                       nn.Tanh(),\n",
    "\t\t                       nn.Linear(flow_artist_shape[0], flow_artist_shape[1]),\n",
    "\t\t                       nn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t nn.Linear(flow_artist_shape[1], flow_artist_shape[2]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t nn.Tanh(),\n",
    "\t\t                       nn.Linear(flow_artist_shape[2], self.embedding_dimension))\n",
    "\t\t# Autoencoder to embed the points into a low dimension\n",
    "\t\tself.encoder = nn.Sequential(nn.Linear(self.data_dimension, autoencoder_shape[0]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[0], autoencoder_shape[1]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[1], self.embedding_dimension))\n",
    "\t\tself.decoder = nn.Sequential(nn.Linear(self.embedding_dimension, autoencoder_shape[1]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[1], autoencoder_shape[0]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[0], self.data_dimension))\n",
    "\t\t# training ops\n",
    "\t\tself.KLD = nn.KLDivLoss(reduction='batchmean',log_target=False)\n",
    "\t\tself.MSE = nn.MSELoss()\n",
    "\t\tself.optim = torch.optim.Adam(self.parameters())\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "\tdef compute_embedding_P(self):\n",
    "\t\tA = affinity_matrix_from_pointset_to_pointset(self.embedded_points,self.embedded_points,flows = self.FlowArtist(self.embedded_points), sigma = self.sigma_embedding, flow_strength=self.flow_strength)\n",
    "\t\t# print(\"affinities \",A)\n",
    "\t\t# flow\n",
    "\t\tself.P_embedding = torch.diag(1/A.sum(axis=1)) @ A\n",
    "\t\t# power it\n",
    "\t\tself.P_embedding_t = torch.matrix_power(self.P_embedding,self.t)\n",
    "\n",
    "\tdef loss(self):\n",
    "\t\tself.embedded_points = self.encoder(self.X)\n",
    "\t\t# print(self.embedded_points)\n",
    "\t\t# compute embedding diffusion matrix\n",
    "\t\tself.compute_embedding_P()\n",
    "\t\t# compute autoencoder loss\n",
    "\t\tX_reconstructed = self.decoder(self.embedded_points)\n",
    "\t\treconstruction_loss = self.MSE(X_reconstructed, self.X)\n",
    "\t\t# print(\"recon loss\",reconstruction_loss)\n",
    "\t\t# take KL divergence between it and actual P\n",
    "\t\t# print(\"embedding p\",self.P_embedding_t)\n",
    "\t\tlog_P_embedding_t = torch.log(self.P_embedding_t)\n",
    "\t\t# print(log_P_embedding_t)\n",
    "\t\tif log_P_embedding_t.is_sparse:\n",
    "\t\t\tdiffusion_loss = self.KLD(log_P_embedding_t.to_dense(),self.P_graph_t.to_dense())\n",
    "\t\telse:\n",
    "\t\t\tdiffusion_loss = self.KLD(log_P_embedding_t,self.P_graph_t)\n",
    "\t\t# print(\"diffusion loss is\",diffusion_loss)\n",
    "\t\tcost = diffusion_loss + reconstruction_loss\n",
    "\t\t# print(f\"cost is KLD {diffusion_loss} with recon {reconstruction_loss}\")\n",
    "\t\tself.losses.append([diffusion_loss,reconstruction_loss])\n",
    "\t\treturn cost\n",
    "\n",
    "\tdef visualize_points(self, labels):\n",
    "\t\t# controls the x and y axes of the plot\n",
    "\t\t# linspace(min on axis, max on axis, spacing on plot -- large number = more field arrows)\n",
    "\t\tminx = min(self.embedded_points[:,0].detach().cpu().numpy())-1\n",
    "\t\tmaxx = max(self.embedded_points[:,0].detach().cpu().numpy())+1\n",
    "\t\tminy = min(self.embedded_points[:,1].detach().cpu().numpy())-1\n",
    "\t\tmaxy = max(self.embedded_points[:,1].detach().cpu().numpy())+1\n",
    "\t\tx, y = np.meshgrid(np.linspace(minx,maxx,20),np.linspace(miny,maxy,20))\n",
    "\t\tx = torch.tensor(x,dtype=float).cpu()\n",
    "\t\ty = torch.tensor(y,dtype=float).cpu()\n",
    "\t\txy_t = torch.concat([x[:,:,None],y[:,:,None]],dim=2).float().to('cuda') # TODO: cuda/cpu issue\n",
    "\t\tuv = self.FlowArtist(xy_t).detach()\n",
    "\t\tu = uv[:,:,0].cpu()\n",
    "\t\tv = uv[:,:,1].cpu()\n",
    "\t\t\n",
    "\t\t# quiver \n",
    "\t\t# \tplots a 2D field of arrows\n",
    "\t\t# \tquiver([X, Y], U, V, [C], **kw); \n",
    "\t\t# \tX, Y define the arrow locations, U, V define the arrow directions, and C optionally sets the color.\n",
    "\t\t\n",
    "\t\tsc = plt.scatter(self.embedded_points[:,0].detach().cpu(),self.embedded_points[:,1].detach().cpu(), c=labels)\n",
    "\t\tplt.quiver(x,y,u,v)\n",
    "\t\tplt.legend()\n",
    "\t\t# Display all open figures.\n",
    "\t\tplt.show()\n",
    "\n",
    "\n",
    "\tdef fit(self,n_steps = 1000):\n",
    "\t\t# train Flow Embedder on the provided graph\n",
    "\t\tself.train()\n",
    "\t\tfor step in trange(n_steps):\n",
    "\t\t\tself.optim.zero_grad()\n",
    "\t\t\t# compute loss\n",
    "\t\t\tloss = self.loss()\n",
    "\t\t\t# print(\"loss is \",loss)\n",
    "\t\t\t# compute gradient and step backwards\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tself.optim.step()\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tif step % 100 == 0:\n",
    "\t\t\t\tprint(f\"EPOCH {step}. Loss {loss}. Flow strength {self.flow_strength}. Heatmap of P embedding is \")\n",
    "\t\t\t\tplt.imshow(self.P_embedding.detach().cpu().numpy())\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\"\"\"\n",
    "\t\t\t# TODO: Criteria to automatically end training\n",
    "\t\tprint(\"Exiting training with loss \",loss)\n",
    "\t\treturn self.embedded_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e35c6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__[:4] == \"1.13\":\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu')\n",
    "else:\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48040c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import *\n",
    "\n",
    "def tree():\n",
    "    labels = []\n",
    "    data = np.array([[None,None] for i in range(900)])\n",
    "    flow = np.array([[None,None] for i in range(900)])\n",
    "    \n",
    "    for i in range(100):\n",
    "        data[i][0] = np.random.uniform(0.0, 1.0)\n",
    "        data[i][1] = np.random.uniform(1.0, 2.0)\n",
    "        f = [np.random.uniform(2.0, 3.0) - data[i][0], np.random.uniform(-1.0, 1.0) - data[i][1]]\n",
    "        flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(0)\n",
    "    for i in range(100,200):\n",
    "        data[i][0] = np.random.uniform(0.0, 1.0)\n",
    "        data[i][1] = np.random.uniform(-2.0, -1.0)\n",
    "        f = [np.random.uniform(2.0, 3.0) - data[i][0], np.random.uniform(-1.0, 1.0) - data[i][1]]\n",
    "        flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(1)\n",
    "    for i in range(200,300):\n",
    "        data[i][0] = np.random.uniform(2.0, 3.0)\n",
    "        data[i][1] = np.random.uniform(-1.0, 1.0)\n",
    "        if(i < 250):\n",
    "            f = [np.random.uniform(4.0, 5.0) - data[i][0], np.random.uniform(2.0, 3.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        else:\n",
    "            f = [np.random.uniform(4.0, 5.0) - data[i][0], np.random.uniform(-2.0, -1.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(2)\n",
    "    for i in range(300,400):\n",
    "        data[i][0] = np.random.uniform(4.0, 5.0)\n",
    "        data[i][1] = np.random.uniform(2.0, 3.0)\n",
    "        if(i < 250):\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(3.0, 4.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        else:\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(1.0, 2.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(3)\n",
    "    for i in range(400,500):\n",
    "        data[i][0] = np.random.uniform(4.0, 5.0)\n",
    "        data[i][1] = np.random.uniform(-2.0, -1.0)\n",
    "        if(i < 250):\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(-2.0, -1.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        else:\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(-4.0, -3.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(4)\n",
    "    for i in range(500,600):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(3.0, 4.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(5)\n",
    "    for i in range(600,700):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(1.0, 2.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(6)\n",
    "    for i in range(700,800):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(-2.0, -1.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(7)\n",
    "    for i in range(800,900):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(-4.0, -3.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(8)\n",
    "        \n",
    "    return np.array(data, dtype=float), np.array(flow, dtype=float), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769110d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, flow, labels = tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data)\n",
    "flow = torch.tensor(flow)\n",
    "X = X.float().to(device)\n",
    "flow = flow.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97716c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (900) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfe \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionFlowEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msigma_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msigma_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dfe \u001b[38;5;241m=\u001b[39m dfe\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m dfe\u001b[38;5;241m.\u001b[39mfit(n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mDiffusionFlowEmbedder.__init__\u001b[0;34m(self, X, flows, t, sigma_graph, sigma_embedding, embedding_dimension, device, autoencoder_shape, flow_artist_shape, flow_strength)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Compute P^t of the graph, the powered diffusion matrix\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# TODO: This can be optimized using landmarks, etc. For now it's straight sparse matrix multiplication\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# TODO: Migrate to a specialized function for dataset affinity calculation, with automatic kernel bandwidth selection, and the like\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_graph \u001b[38;5;241m=\u001b[39m \u001b[43maffinity_matrix_from_pointset_to_pointset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mflows\u001b[49m\u001b[43m,\u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_graph_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatrix_power(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_graph,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Flow field\u001b[39;00m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36maffinity_matrix_from_pointset_to_pointset\u001b[0;34m(pointset1, pointset2, flows, n_neighbors, sigma, flow_strength)\u001b[0m\n\u001b[1;32m     23\u001b[0m P3 \u001b[38;5;241m=\u001b[39m P3\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# compute affinities from flows and directions\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m affinities \u001b[38;5;241m=\u001b[39m \u001b[43maffinity_from_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflows\u001b[49m\u001b[43m,\u001b[49m\u001b[43mP3\u001b[49m\u001b[43m,\u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mflow_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflow_strength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m affinities\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36maffinity_from_flow\u001b[0;34m(flows, directions_array, flow_strength, sigma)\u001b[0m\n\u001b[1;32m     27\u001b[0m dot_products \u001b[38;5;241m=\u001b[39m (normed_directions \u001b[38;5;241m*\u001b[39m flows)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# take distance between flow projected onto direction and the direction\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m distance_from_flow \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mflows\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_directions\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdot_products\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# take absolute value\u001b[39;00m\n\u001b[1;32m     31\u001b[0m distance_from_flow \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(distance_from_flow)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (900) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "dfe = DiffusionFlowEmbedder(X,flow,t=1,sigma_graph=15,sigma_embedding=15)\n",
    "dfe = dfe.to(device)\n",
    "embeddings = dfe.fit(n_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952be2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe.visualize_points(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8a8e5",
   "metadata": {},
   "source": [
    "### On circle and Swiss Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from directed_graphs.datasets import directed_circle, directed_cylinder, directed_spiral, directed_swiss_roll\n",
    "from directed_graphs.datasets import plot_directed_2d, plot_directed_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3819bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, flow, labels = directed_circle(num_nodes=2000, radius=1)\n",
    "plot_directed_2d(X, flow, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)\n",
    "flow = torch.tensor(flow)\n",
    "X = X.float().to(device)\n",
    "flow = flow.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = DiffusionFlowEmbedder(X,flow,t=1,sigma_graph=15,sigma_embedding=15,device=device)\n",
    "dfe = dfe.to(device)\n",
    "embeddings = dfe.fit(n_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe.visualize_points(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
