{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5fd9c",
   "metadata": {},
   "source": [
    "### Kernel -- Calculate Affinities from Points and Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def calculate_distance(points):\n",
    "    return distance_matrix(points, points)\n",
    "\n",
    "def normalize(v):\n",
    "    v = np.asarray(v)\n",
    "    if np.sqrt(v.dot(v)) == 0:\n",
    "        return v\n",
    "    else:\n",
    "        return v/np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5355d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors_to_matrix(points):\n",
    "    points = np.asarray(points)\n",
    "    n_pts = len(points)\n",
    "    \n",
    "    vecs_to = [[None for i in range(n_pts)] for j in range(n_pts)]\n",
    "    \n",
    "    for i in range(n_pts):\n",
    "        for j in range(n_pts):\n",
    "            vecs_to[i][j] = points[j] - points[i]\n",
    "            \n",
    "    return vecs_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([0, 0]), array([-1,  1])], [array([ 1, -1]), array([0, 0])]]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "X = [[1,0], \n",
    "     [0,1]]\n",
    "\n",
    "vecs_to = calculate_vectors_to(X)\n",
    "print(vecs_to)\n",
    "print(vecs_to[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(x1,x2,flow, eps=1):\n",
    "    x1 = np.asarray(x1)\n",
    "    x2 = np.asarray(x2)\n",
    "    flow = np.asarray(flow)\n",
    "    \n",
    "    vec = x2-x1\n",
    "    flow_assist = 1+eps-np.dot(normalize(flow), normalize(vec))\n",
    "    vec_mag = np.sqrt(vec.dot(vec))\n",
    "    flow_mag = np.sqrt(flow.dot(flow))\n",
    "    \n",
    "    cost = flow_assist*vec_mag/flow_mag\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cf309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.70710678,  0.70710678],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [-0.70710678,  0.70710678],\n",
       "       [-1.        ,  0.        ],\n",
       "       [-0.70710678, -0.70710678],\n",
       "       [ 0.        , -1.        ],\n",
       "       [ 0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_matrix():\n",
    "    ret = []\n",
    "    ret.append([0,0])\n",
    "    ret.append(normalize([1,0]))\n",
    "    ret.append(normalize([1,1]))\n",
    "    ret.append(normalize([0,1]))\n",
    "    ret.append(normalize([-1,1]))\n",
    "    ret.append(normalize([-1,0]))\n",
    "    ret.append(normalize([-1,-1]))\n",
    "    ret.append(normalize([0,-1]))\n",
    "    ret.append(normalize([1,-1]))\n",
    "    \n",
    "    return np.multiply(1,ret)\n",
    "\n",
    "test_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeab618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.2928932188134523, 2.0, 2.707106781186547, 3.0, 2.707106781186547, 2.0, 1.2928932188134523]\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    mat = test_matrix()\n",
    "    costs = []\n",
    "    for i in range(len(mat)):\n",
    "        costs.append(calculate_cost(mat[0], mat[i], flow = [1,0]))\n",
    "        \n",
    "    return costs\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe562b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_matrix(points, flow, ep=1):\n",
    "    points = np.asarray(points)\n",
    "    n_pts = len(points)\n",
    "    \n",
    "    cost = [[None for i in range(n_pts)] for j in range(n_pts)]\n",
    "    \n",
    "    for i in range(n_pts):\n",
    "        for j in range(n_pts):\n",
    "            cost[i][j] = calculate_cost(points[i], points[j], flow[i],eps)\n",
    "            \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e97a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cost_matrix(data, flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_affinity_matrix(points, flow, sigma):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c51623",
   "metadata": {},
   "source": [
    "## Previous Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def affinity_from_flow(flows, directions_array, flow_strength = 1, sigma=1):\n",
    "  \"\"\"Compute probabilities of transition in the given directions based on the flow. \n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  flows : torch tensor of shape n_points x n_dims\n",
    "      _description_\n",
    "  directions_array : torch tensor of shape n_directions x n_points x n_dims. Assumed to be normalized.\n",
    "      _description_\n",
    "  sigma : int, optional\n",
    "      kernel bandwidth, by default 1\n",
    "  returns (n_points)\n",
    "  \"\"\"\n",
    "  assert len(flows.shape) == 2 # flows should only have one dimension\n",
    "  assert len(directions_array.shape) > 1 and len(directions_array.shape) < 4\n",
    "  n_directions = directions_array.shape[0]\n",
    "  # Normalize directions\n",
    "  length_of_directions = torch.linalg.norm(directions_array,dim=-1)\n",
    "#  normed_directions = F.normalize(directions_array,dim=-1)\n",
    "  # and normalize flows # TODO: Perhaps reconsider\n",
    "#  flows = F.normalize(flows,dim=-1)\n",
    "\n",
    "  if len(directions_array) == 1: # convert to 2d array if necessary\n",
    "    directions_array = directions_array[:,None] \n",
    "  # compute dot products as matrix multiplication\n",
    "  dot_products = (normed_directions * flows).sum(-1)\n",
    "  # take distance between flow projected onto direction and the direction\n",
    "  distance_from_flow = (torch.linalg.norm(flows,dim=1)**2).repeat(n_directions,1) - dot_products\n",
    "  # take absolute value\n",
    "  distance_from_flow = torch.abs(distance_from_flow)\n",
    "  # print('shape of dff',distance_from_flow.shape)\n",
    "  # add to this the length of each direction\n",
    "  distance_from_flow = flow_strength*distance_from_flow + length_of_directions\n",
    "  # put the points on rows, directions in columns\n",
    "  distance_from_flow = distance_from_flow.T\n",
    "  # take kernel of distances\n",
    "  kernel =  torch.exp(-distance_from_flow/sigma)\n",
    "  # normalize kernel\n",
    "  # kernel /= torch.sum(kernel,axis=1)\n",
    "  return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54703c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "def affinity_from_flow(flows, directions_array, flow_strength = 1, sigma=1):\n",
    "  \"\"\"Compute probabilities of transition in the given directions based on the flow. \n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  flows : torch tensor of shape n_points x n_dims\n",
    "      _description_\n",
    "  directions_array : torch tensor of shape n_directions x n_points x n_dims. Assumed to be normalized.\n",
    "      _description_\n",
    "  sigma : int, optional\n",
    "      kernel bandwidth, by default 1\n",
    "  returns (n_points)\n",
    "  \"\"\"\n",
    "  assert len(flows.shape) == 2 # flows should only have one dimension\n",
    "  assert len(directions_array.shape) > 1 and len(directions_array.shape) < 4\n",
    "  n_directions = directions_array.shape[0]\n",
    "  # Normalize directions\n",
    "  length_of_directions = torch.linalg.norm(directions_array,dim=-1)\n",
    "  normed_directions = F.normalize(directions_array,dim=-1)\n",
    "  # and normalize flows # TODO: Perhaps reconsider\n",
    "  flows = F.normalize(flows,dim=-1)\n",
    "\n",
    "  if len(directions_array) == 1: # convert to 2d array if necessary\n",
    "    directions_array = directions_array[:,None] \n",
    "  # compute dot products as matrix multiplication\n",
    "  dot_products = (normed_directions * flows).sum(-1)\n",
    "  # take distance between flow projected onto direction and the direction\n",
    "  distance_from_flow = (torch.linalg.norm(flows,dim=1)**2).repeat(n_directions,1) - dot_products\n",
    "  # take absolute value\n",
    "  distance_from_flow = torch.abs(distance_from_flow)\n",
    "  # print('shape of dff',distance_from_flow.shape)\n",
    "  # add to this the length of each direction\n",
    "  distance_from_flow = flow_strength*distance_from_flow + length_of_directions\n",
    "  # put the points on rows, directions in columns\n",
    "  distance_from_flow = distance_from_flow.T\n",
    "  # take kernel of distances\n",
    "  kernel =  torch.exp(-distance_from_flow/sigma)\n",
    "  # normalize kernel\n",
    "  # kernel /= torch.sum(kernel,axis=1)\n",
    "  return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b54d03",
   "metadata": {},
   "source": [
    "### Calculate Directions between point i to point j and apply Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affinity_matrix_from_pointset_to_pointset(pointset1, pointset2, flows,n_neighbors=None,sigma=0.5, flow_strength=1):\n",
    "  \"\"\"Compute affinity matrix between the points of pointset1 and pointset2, using the provided flow.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  pointset1 : torch tensor, n1 x d\n",
    "      The first pointset, to calculate affinities *from*\n",
    "  pointset2 : torch tensor, n2 x d\n",
    "      The second pointset, to calculate affinities *to* (from pointset1)\n",
    "  flow : a function that, when called at a point, gives the flow at that point\n",
    "  n_neighbors : number of neighbors to include in affinity computations. All neighbors beyond it are given affinity zero\n",
    "  (currently not implemented)\n",
    "\n",
    "  Returns:\n",
    "  Affinity matrix: torch tensor of shape n1 x n2\n",
    "  \"\"\"\n",
    "  # Calculate the directions from point i in pointset 1 to point j in pointset 2\n",
    "  n1 = pointset1.shape[0]\n",
    "  n2 = pointset2.shape[0]\n",
    "  P2 = pointset2[:,:,None].repeat(1,1,n1)\n",
    "  P1 = pointset1.T.repeat(n2,1,1)\n",
    "  P3 = (P2 - P1)\n",
    "  P3 = P3.transpose(1,2)\n",
    "  # compute affinities from flows and directions\n",
    "  affinities = affinity_from_flow(flows,P3,sigma=sigma,flow_strength=flow_strength)\n",
    "  return affinities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0902f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from tqdm import trange\n",
    "from directed_graphs.utils import diffusion_matrix_from_graph\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DiffusionFlowEmbedder(torch.nn.Module):\n",
    "\tdef __init__(self, X, flows, t = 4, sigma_graph = 0.5, sigma_embedding=0.5, embedding_dimension=2, device=torch.device('cpu'), autoencoder_shape = [100,10], flow_artist_shape = [10,20,10], flow_strength=1):\n",
    "\t\t\"\"\"Flow Embedding with diffusion\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tX : torch tensor n_points x n_dim\n",
    "\t\t\tdata matrix\n",
    "\t\tflows : torch tensor n_points x n_dim\n",
    "\t\t\tThe flow at each point\n",
    "\t\tt : int\n",
    "\t\t\tLoss is computed with the diffusion operator powered to this number\n",
    "\t\tsigma in [0,1]\n",
    "\t\t\tKernel bandwidth in the embedding\n",
    "\t\t\"\"\"\n",
    "\t\t# initialize parameters\n",
    "\t\tsuper(DiffusionFlowEmbedder, self).__init__()\n",
    "\t\tself.X = X\n",
    "\t\tself.ground_truth_flows = flows\n",
    "\t\tself.t = t\n",
    "\t\tself.sigma_embedding = sigma_embedding\n",
    "\t\tself.sigma_graph = sigma_graph\n",
    "\t\tself.nnodes = X.shape[0]\n",
    "\t\tself.data_dimension = X.shape[1]\n",
    "\t\tself.losses = []\n",
    "\t\tself.eps = 0.001\n",
    "\t\tif flow_strength == \"learnable\":\n",
    "\t\t\tself.flow_strength = nn.Parameter(torch.tensor(1.0))\n",
    "\t\telse:\n",
    "\t\t\tself.flow_strength = flow_strength\t\n",
    "\n",
    "\t\tself.embedding_dimension = embedding_dimension\n",
    "\t\t# set device (used for shuffling points around during visualization)\n",
    "\t\tself.device = device\n",
    "\t\t# Compute P^t of the graph, the powered diffusion matrix\n",
    "\t\t# TODO: This can be optimized using landmarks, etc. For now it's straight sparse matrix multiplication\n",
    "\t\t# TODO: Migrate to a specialized function for dataset affinity calculation, with automatic kernel bandwidth selection, and the like\n",
    "\t\tself.P_graph = affinity_matrix_from_pointset_to_pointset(X,X,flows,sigma=sigma_graph)\n",
    "\t\tself.P_graph_t = torch.matrix_power(self.P_graph,self.t)\n",
    "\t\t# Flow field\n",
    "\t\tself.FlowArtist = nn.Sequential(nn.Linear(self.embedding_dimension, flow_artist_shape[0]),\n",
    "\t\t                       nn.Tanh(),\n",
    "\t\t                       nn.Linear(flow_artist_shape[0], flow_artist_shape[1]),\n",
    "\t\t                       nn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t nn.Linear(flow_artist_shape[1], flow_artist_shape[2]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t nn.Tanh(),\n",
    "\t\t                       nn.Linear(flow_artist_shape[2], self.embedding_dimension))\n",
    "\t\t# Autoencoder to embed the points into a low dimension\n",
    "\t\tself.encoder = nn.Sequential(nn.Linear(self.data_dimension, autoencoder_shape[0]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[0], autoencoder_shape[1]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[1], self.embedding_dimension))\n",
    "\t\tself.decoder = nn.Sequential(nn.Linear(self.embedding_dimension, autoencoder_shape[1]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[1], autoencoder_shape[0]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Linear(autoencoder_shape[0], self.data_dimension))\n",
    "\t\t# training ops\n",
    "\t\tself.KLD = nn.KLDivLoss(reduction='batchmean',log_target=False)\n",
    "\t\tself.MSE = nn.MSELoss()\n",
    "\t\tself.optim = torch.optim.Adam(self.parameters())\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "\tdef compute_embedding_P(self):\n",
    "\t\tA = affinity_matrix_from_pointset_to_pointset(self.embedded_points,self.embedded_points,flows = self.FlowArtist(self.embedded_points), sigma = self.sigma_embedding, flow_strength=self.flow_strength)\n",
    "\t\t# print(\"affinities \",A)\n",
    "\t\t# flow\n",
    "\t\tself.P_embedding = torch.diag(1/A.sum(axis=1)) @ A\n",
    "\t\t# power it\n",
    "\t\tself.P_embedding_t = torch.matrix_power(self.P_embedding,self.t)\n",
    "\n",
    "\tdef loss(self):\n",
    "\t\tself.embedded_points = self.encoder(self.X)\n",
    "\t\t# print(self.embedded_points)\n",
    "\t\t# compute embedding diffusion matrix\n",
    "\t\tself.compute_embedding_P()\n",
    "\t\t# compute autoencoder loss\n",
    "\t\tX_reconstructed = self.decoder(self.embedded_points)\n",
    "\t\treconstruction_loss = self.MSE(X_reconstructed, self.X)\n",
    "\t\t# print(\"recon loss\",reconstruction_loss)\n",
    "\t\t# take KL divergence between it and actual P\n",
    "\t\t# print(\"embedding p\",self.P_embedding_t)\n",
    "\t\tlog_P_embedding_t = torch.log(self.P_embedding_t)\n",
    "\t\t# print(log_P_embedding_t)\n",
    "\t\tif log_P_embedding_t.is_sparse:\n",
    "\t\t\tdiffusion_loss = self.KLD(log_P_embedding_t.to_dense(),self.P_graph_t.to_dense())\n",
    "\t\telse:\n",
    "\t\t\tdiffusion_loss = self.KLD(log_P_embedding_t,self.P_graph_t)\n",
    "\t\t# print(\"diffusion loss is\",diffusion_loss)\n",
    "\t\tcost = diffusion_loss + reconstruction_loss\n",
    "\t\t# print(f\"cost is KLD {diffusion_loss} with recon {reconstruction_loss}\")\n",
    "\t\tself.losses.append([diffusion_loss,reconstruction_loss])\n",
    "\t\treturn cost\n",
    "\n",
    "\tdef visualize_points(self, labels):\n",
    "\t\t# controls the x and y axes of the plot\n",
    "\t\t# linspace(min on axis, max on axis, spacing on plot -- large number = more field arrows)\n",
    "\t\tminx = min(self.embedded_points[:,0].detach().cpu().numpy())-1\n",
    "\t\tmaxx = max(self.embedded_points[:,0].detach().cpu().numpy())+1\n",
    "\t\tminy = min(self.embedded_points[:,1].detach().cpu().numpy())-1\n",
    "\t\tmaxy = max(self.embedded_points[:,1].detach().cpu().numpy())+1\n",
    "\t\tx, y = np.meshgrid(np.linspace(minx,maxx,20),np.linspace(miny,maxy,20))\n",
    "\t\tx = torch.tensor(x,dtype=float).cpu()\n",
    "\t\ty = torch.tensor(y,dtype=float).cpu()\n",
    "\t\txy_t = torch.concat([x[:,:,None],y[:,:,None]],dim=2).float().to('cuda') # TODO: cuda/cpu issue\n",
    "\t\tuv = self.FlowArtist(xy_t).detach()\n",
    "\t\tu = uv[:,:,0].cpu()\n",
    "\t\tv = uv[:,:,1].cpu()\n",
    "\t\t\n",
    "\t\t# quiver \n",
    "\t\t# \tplots a 2D field of arrows\n",
    "\t\t# \tquiver([X, Y], U, V, [C], **kw); \n",
    "\t\t# \tX, Y define the arrow locations, U, V define the arrow directions, and C optionally sets the color.\n",
    "\t\t\n",
    "\t\tsc = plt.scatter(self.embedded_points[:,0].detach().cpu(),self.embedded_points[:,1].detach().cpu(), c=labels)\n",
    "\t\tplt.quiver(x,y,u,v)\n",
    "\t\tplt.legend()\n",
    "\t\t# Display all open figures.\n",
    "\t\tplt.show()\n",
    "\n",
    "\n",
    "\tdef fit(self,n_steps = 1000):\n",
    "\t\t# train Flow Embedder on the provided graph\n",
    "\t\tself.train()\n",
    "\t\tfor step in trange(n_steps):\n",
    "\t\t\tself.optim.zero_grad()\n",
    "\t\t\t# compute loss\n",
    "\t\t\tloss = self.loss()\n",
    "\t\t\t# print(\"loss is \",loss)\n",
    "\t\t\t# compute gradient and step backwards\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tself.optim.step()\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tif step % 100 == 0:\n",
    "\t\t\t\tprint(f\"EPOCH {step}. Loss {loss}. Flow strength {self.flow_strength}. Heatmap of P embedding is \")\n",
    "\t\t\t\tplt.imshow(self.P_embedding.detach().cpu().numpy())\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\"\"\"\n",
    "\t\t\t# TODO: Criteria to automatically end training\n",
    "\t\tprint(\"Exiting training with loss \",loss)\n",
    "\t\treturn self.embedded_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e35c6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__[:4] == \"1.13\":\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu')\n",
    "else:\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48040c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import *\n",
    "\n",
    "def tree():\n",
    "    labels = []\n",
    "    data = np.array([[None,None] for i in range(900)])\n",
    "    flow = np.array([[None,None] for i in range(900)])\n",
    "    \n",
    "    for i in range(100):\n",
    "        data[i][0] = np.random.uniform(0.0, 1.0)\n",
    "        data[i][1] = np.random.uniform(1.0, 2.0)\n",
    "        f = [np.random.uniform(2.0, 3.0) - data[i][0], np.random.uniform(-1.0, 1.0) - data[i][1]]\n",
    "        flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(0)\n",
    "    for i in range(100,200):\n",
    "        data[i][0] = np.random.uniform(0.0, 1.0)\n",
    "        data[i][1] = np.random.uniform(-2.0, -1.0)\n",
    "        f = [np.random.uniform(2.0, 3.0) - data[i][0], np.random.uniform(-1.0, 1.0) - data[i][1]]\n",
    "        flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(1)\n",
    "    for i in range(200,300):\n",
    "        data[i][0] = np.random.uniform(2.0, 3.0)\n",
    "        data[i][1] = np.random.uniform(-1.0, 1.0)\n",
    "        if(i < 250):\n",
    "            f = [np.random.uniform(4.0, 5.0) - data[i][0], np.random.uniform(2.0, 3.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        else:\n",
    "            f = [np.random.uniform(4.0, 5.0) - data[i][0], np.random.uniform(-2.0, -1.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(2)\n",
    "    for i in range(300,400):\n",
    "        data[i][0] = np.random.uniform(4.0, 5.0)\n",
    "        data[i][1] = np.random.uniform(2.0, 3.0)\n",
    "        if(i < 250):\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(3.0, 4.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        else:\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(1.0, 2.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(3)\n",
    "    for i in range(400,500):\n",
    "        data[i][0] = np.random.uniform(4.0, 5.0)\n",
    "        data[i][1] = np.random.uniform(-2.0, -1.0)\n",
    "        if(i < 250):\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(-2.0, -1.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        else:\n",
    "            f = [np.random.uniform(6.0, 7.0) - data[i][0], np.random.uniform(-4.0, -3.0) - data[i][1]]\n",
    "            flow[i] = f/np.linalg.norm(f)\n",
    "        labels.append(4)\n",
    "    for i in range(500,600):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(3.0, 4.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(5)\n",
    "    for i in range(600,700):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(1.0, 2.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(6)\n",
    "    for i in range(700,800):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(-2.0, -1.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(7)\n",
    "    for i in range(800,900):\n",
    "        data[i][0] = np.random.uniform(6.0, 7.0)\n",
    "        data[i][1] = np.random.uniform(-4.0, -3.0)\n",
    "        flow[i] = [1,0]\n",
    "        labels.append(8)\n",
    "        \n",
    "    return np.array(data, dtype=float), np.array(flow, dtype=float), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769110d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, flow, labels = tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data)\n",
    "flow = torch.tensor(flow)\n",
    "X = X.float().to(device)\n",
    "flow = flow.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = DiffusionFlowEmbedder(X,flow,t=1,sigma_graph=15,sigma_embedding=15)\n",
    "dfe = dfe.to(device)\n",
    "embeddings = dfe.fit(n_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952be2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe.visualize_points(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8a8e5",
   "metadata": {},
   "source": [
    "### On circle and Swiss Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from directed_graphs.datasets import directed_circle, directed_cylinder, directed_spiral, directed_swiss_roll\n",
    "from directed_graphs.datasets import plot_directed_2d, plot_directed_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3819bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, flow, labels = directed_circle(num_nodes=2000, radius=1)\n",
    "plot_directed_2d(X, flow, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)\n",
    "flow = torch.tensor(flow)\n",
    "X = X.float().to(device)\n",
    "flow = flow.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = DiffusionFlowEmbedder(X,flow,t=1,sigma_graph=15,sigma_embedding=15,device=device)\n",
    "dfe = dfe.to(device)\n",
    "embeddings = dfe.fit(n_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe.visualize_points(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
