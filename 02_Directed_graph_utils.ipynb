{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Graph Utils\n",
    "\n",
    "Utility functions to calculate the magnetic laplacian and perform other common tasks in directed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp utils\n",
    "from nbdev.showdoc import *\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnetic Laplacian (unfinished)\n",
    "> Note: this currently attempts to create a *sparse* laplacian, but encounters difficulties working with complex numbers in sparse matrices.\n",
    "\n",
    "The Magnetic Laplacian is an adaptation of normal Graph Laplacian to the setting of undirected graphs. Via a tunable parameter $q$, it allows manual specification of the level of undirected information one wishes to incorporate when training their models. Although the Magnetic Laplacian is only one among several means of extending the laplacian to directed graphs, it also appears \"in nature\", dating back to physics research in the 1990s.\n",
    "\n",
    "The Magnetic Laplacian is defined through several steps, which we will walk through in turn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a PyG dataset\n",
    "\n",
    "Perlmutter et al. found the WebKB \"Texas\" and \"Wisconsin\" datasets amenable to directed graph pursuits. Each dataset describes the links between the web pages of a university, including student, class, faculty, and project webpages. As it is available through PyG, using it should (one hopes) be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import WebKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WebKB('~/data',name = \"Texas\", transform=None, pre_transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's only one graph in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 183\n",
      "Number of edges: 325\n",
      "Average node degree: 1.78\n",
      "Number of training nodes: 870\n",
      "Training node label rate: 4.75\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pyg graph objects have several standard features:\n",
    "1. `.x` gives the node features, in rows\n",
    "2. `.edge_index` gives the adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.edge_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the `edge_index` encodes graph connectivity in two long arrays, where a1[i] -> a2[i]. This is a sparse matrix format known as COO. Presently, we just care about the connectivity of the graph, so we'll separate this data into its own sparse tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.sparse\n",
    "A_directed = torch.sparse_coo_tensor(data.edge_index, torch.ones(data.num_edges),(data.num_nodes,data.num_nodes) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to preview what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_directed.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We form the *symmetrized adjacency matrix* as you'd expect, by averaging the two directions. Purely outgoing connections become 1/2 strength connections, while bidirectional links remain weighted as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_symmetrized = 1/2*(A_directed + torch.t(A_directed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sparse\n",
    "Degree = sparse.sum(A_symmetrized, dim = [1]).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  0.5000,  0.5000,  0.5000,  1.5000,  1.5000,  1.0000,  1.0000,\n",
       "         1.0000,  1.0000,  0.5000,  2.0000,  0.5000,  2.0000,  0.5000,  5.0000,\n",
       "         4.5000,  1.5000,  1.5000,  0.5000,  2.0000,  1.5000,  2.0000,  2.5000,\n",
       "         1.5000,  1.0000,  0.5000,  0.5000,  1.0000,  4.0000,  1.5000,  1.5000,\n",
       "         0.5000,  0.5000,  5.0000,  0.5000,  1.0000,  1.0000,  0.5000,  2.0000,\n",
       "         0.5000,  2.5000,  1.0000,  0.5000,  1.5000,  2.0000,  1.5000,  2.5000,\n",
       "         1.0000,  1.0000,  1.5000,  0.5000,  0.5000,  1.0000,  1.5000,  2.0000,\n",
       "        52.0000,  4.0000,  6.0000,  2.0000,  1.5000,  1.0000,  1.5000,  1.0000,\n",
       "         2.0000,  1.5000,  6.5000,  1.5000,  1.0000,  0.5000,  0.5000,  0.5000,\n",
       "         1.0000,  2.5000,  1.5000,  1.0000,  1.0000,  0.5000,  1.0000,  2.0000,\n",
       "         2.5000,  1.0000,  4.5000,  2.0000, 10.0000,  1.5000,  1.5000,  1.0000,\n",
       "         1.0000,  1.5000,  2.5000,  0.5000,  0.5000,  1.0000,  1.5000,  3.5000,\n",
       "         0.5000,  1.0000,  0.5000,  1.5000,  1.0000,  0.5000,  3.0000,  0.5000,\n",
       "         1.0000,  0.5000,  0.5000,  0.5000,  4.0000,  0.5000,  1.0000,  0.5000,\n",
       "         0.5000,  1.0000,  0.5000,  0.5000,  4.5000,  1.0000,  1.0000,  1.5000,\n",
       "         1.0000,  0.5000,  0.5000,  0.5000,  0.5000,  1.5000,  2.0000,  5.5000,\n",
       "         0.5000,  1.5000,  0.5000,  4.5000,  1.0000,  2.5000,  1.5000,  0.5000,\n",
       "         0.5000,  0.5000,  0.5000,  1.0000,  3.5000,  1.0000,  0.5000,  0.5000,\n",
       "         2.0000,  1.0000,  4.0000,  2.0000,  2.0000,  0.5000,  1.5000,  2.0000,\n",
       "         0.5000,  1.5000,  0.5000,  0.5000,  2.0000,  0.5000,  1.0000,  4.0000,\n",
       "         0.5000,  1.0000,  1.5000,  2.0000,  1.0000,  1.5000,  0.5000,  2.0000,\n",
       "         1.5000,  0.5000,  1.0000,  3.5000,  1.0000,  3.5000,  0.5000,  1.5000,\n",
       "         0.5000,  1.5000,  1.0000,  0.5000,  3.0000,  0.5000,  0.5000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we define the *phase* matrix to capture the directional information so cruelly discarded by `A_symmetrized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.25\n",
    "Phase_matrix = 2*torch.pi*q*(A_directed - torch.t(A_directed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversion into Torch's Complex Numbers\n",
    "Torch tensors have (beta) support for complex types, using the datatype `cfloat`. These can be instantiated with the function `torch.complex`, and, fortunately, the datatypes carry over intuitively: multiplication of real and complex values preserves the complex values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = torch.complex(torch.zeros(1),torch.ones(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.+0.9586j, 0.+0.1400j, 0.+0.8984j],\n",
       "        [0.+0.7221j, 0.+0.7698j, 0.+0.2241j],\n",
       "        [0.+0.1167j, 0.+0.7521j, 0.+0.5159j]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j * torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Pytorch doesn't currently support scalar multiplication between a sparse matrix and a complex number. We have to do it the long way. Notably, we have to convert the Phase_matrix to have dtype of complex float, otherwise strange errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phase_matrix = Phase_matrix.cfloat()\n",
    "imaginary_phase_matrix = torch.smm(Phase_matrix, torch.diag(torch.complex(torch.zeros(len(Phase_matrix)),torch.ones(len(Phase_matrix)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  0,   0,   1,  ..., 180,  29, 182],\n",
       "                       [ 58, 121,  80,  ..., 180, 182,  29]]),\n",
       "       values=tensor([ 1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j, -1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j, -1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j,  1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  0.0000+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,\n",
       "                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,  1.5708+0.j]),\n",
       "       size=(183, 183), nnz=649, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phase_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the exponential is not defined element-wise for sparse matrices, but we can do point-wise multiplication via the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase_matrix = Phase_matrix.coalesce()\n",
    "# v_phase = imaginary_phase_matrix.coalesce().values()\n",
    "# v_phase = torch.exp(v_phase)\n",
    "# Phase_matrix_expd = torch.sparse_coo_tensor(Phase_matrix.indices(),v_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(v_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.sum(v_phase[500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Diffusion Matrix\n",
    "- [ ] Add support for edge weights\n",
    "\n",
    "Util to take a directed adjacency matrix, and normalize it to a diffusion matrix. \n",
    "\n",
    "It accepts either sparse adjacency matrices, or PyG graphs, assuming torch's COO sparse format. Indeed, given a non-sparse adjacency matrix, it converts the matrix to a COO sparse matrix before continuing with the normalization\n",
    "\n",
    "$$ P = D^{-1} A $$\n",
    "\n",
    "The only difficulty lays in normalization: directed graphs may have rows with zero sum. To counteract that, we use a normalization function which takes the rowsums, and returns an indicator vector: 1 if zero, 0 otherwise. This is done in a slightly hacky way, by taking\n",
    "\n",
    "$$ \\text{floor}(2^{-rowsums}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch import sparse\n",
    "def diffusion_matrix_from_graph(A = None, G = None, self_loops=5):\n",
    "  \"\"\"\n",
    "  Given directed adjacency matrix (sparse or unsparse), returns sparse diffusion matrix.\n",
    "  Accepts tensor inputs of `A`, in COO sparse form, or dense, or can work directly from a PyG graph, given via argument `G`.\n",
    "  \"\"\"\n",
    "  if G is not None:\n",
    "    # We were given a graph. Extract the indices and values from it:\n",
    "    A = torch.sparse_coo_tensor(G.edge_index, torch.ones(G.num_edges),(G.num_nodes,G.num_nodes) )\n",
    "  if A is not None:\n",
    "    # check if A is sparse\n",
    "    if not A.is_sparse:\n",
    "      A = A.to_sparse()\n",
    "    if self_loops > 0:\n",
    "      A = A + (self_loops * torch.eye(A.shape[0])).to_sparse()\n",
    "    # We now have a sparse tensor: get row sums and set zeros equal to one\n",
    "    # this prevents division by zero errors\n",
    "    degree = sparse.sum(A, dim=[1]).to_dense()\n",
    "    degree[degree == 0] = 1\n",
    "    one_over_degree = 1 / degree\n",
    "    D_negative_one = torch.diag(one_over_degree).to_sparse()\n",
    "    # Row normalize by multiplying with a diagonal matrix\n",
    "    P = D_negative_one @ A\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll ensure this works with a few tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Does it work on matrix data?\n",
    "X = torch.rand(20,20)\n",
    "P = diffusion_matrix_from_graph(A = X)\n",
    "# the maximum rowsum should be one, and the min should be either one or zero\n",
    "max_row_sum = max(sparse.sum(P, dim=[1]).to_dense())\n",
    "print(max_row_sum)\n",
    "assert torch.allclose(max_row_sum, torch.ones(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Does it work on graph data?\n",
    "P = diffusion_matrix_from_graph(G = data)\n",
    "max_row_sum = max(sparse.sum(P, dim=[1]).to_dense())\n",
    "print(max_row_sum)\n",
    "assert torch.allclose(max_row_sum, torch.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(10).to_sparse().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_Diffusion Curvature of Directed Graphs.ipynb.\n",
      "Converted 02_Directed_graph_utils.ipynb.\n",
      "Converted 03_Toy_Graph_Datasets.ipynb.\n",
      "Converted 12_differentiable_diffusion_curvature.ipynb.\n",
      "Converted 21_Communities_Datasets.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /Users/adjourner/Projects/directed_graphs/02_Directed_graph_utils.ipynb\n",
      "converting /Users/adjourner/Projects/directed_graphs/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pyg_from_source')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
