---

title: Directed Graph Utils


keywords: fastai
sidebar: home_sidebar



nb_path: "02_Directed_graph_utils.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 02_Directed_graph_utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Magnetic-Laplacian-(unfinished)">Magnetic Laplacian (unfinished)<a class="anchor-link" href="#Magnetic-Laplacian-(unfinished)"> </a></h1><p>{% include note.html content='this currently attempts to create a <em>sparse</em> laplacian, but encounters difficulties working with complex numbers in sparse matrices.' %}
The Magnetic Laplacian is an adaptation of normal Graph Laplacian to the setting of undirected graphs. Via a tunable parameter $q$, it allows manual specification of the level of undirected information one wishes to incorporate when training their models. Although the Magnetic Laplacian is only one among several means of extending the laplacian to directed graphs, it also appears "in nature", dating back to physics research in the 1990s.</p>
<p>The Magnetic Laplacian is defined through several steps, which we will walk through in turn.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Establishing-a-PyG-dataset">Establishing a PyG dataset<a class="anchor-link" href="#Establishing-a-PyG-dataset"> </a></h2><p>Perlmutter et al. found the WebKB "Texas" and "Wisconsin" datasets amenable to directed graph pursuits. Each dataset describes the links between the web pages of a university, including student, class, faculty, and project webpages. As it is available through PyG, using it should (one hopes) be easy.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">WebKB</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">WebKB</span><span class="p">(</span><span class="s1">&#39;~/data&#39;</span><span class="p">,</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Texas&quot;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pre_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There's only one graph in this dataset</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, let's extract it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of nodes: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of edges: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">num_edges</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Average node degree: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">num_edges</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of training nodes: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Training node label rate: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">train_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Has isolated nodes: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">has_isolated_nodes</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Has self-loops: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">has_self_loops</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Is undirected: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">is_undirected</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of nodes: 183
Number of edges: 325
Average node degree: 1.78
Number of training nodes: 870
Training node label rate: 4.75
Has isolated nodes: False
Has self-loops: True
Is undirected: False
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The pyg graph objects have several standard features:</p>
<ol>
<li><code>.x</code> gives the node features, in rows</li>
<li><code>.edge_index</code> gives the adjacency</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>325</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As seen above, the <code>edge_index</code> encodes graph connectivity in two long arrays, where a1[i] -&gt; a2[i]. This is a sparse matrix format known as COO. Presently, we just care about the connectivity of the graph, so we'll separate this data into its own sparse tensor:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.sparse</span>
<span class="n">A_directed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_edges</span><span class="p">),(</span><span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">,</span><span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">)</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just to preview what this looks like:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">A_directed</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 1., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We form the <em>symmetrized adjacency matrix</em> as you'd expect, by averaging the two directions. Purely outgoing connections become 1/2 strength connections, while bidirectional links remain weighted as 1.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">A_symmetrized</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">A_directed</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">A_directed</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">sparse</span>
<span class="n">Degree</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A_symmetrized</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Degree</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 1.0000,  0.5000,  0.5000,  0.5000,  1.5000,  1.5000,  1.0000,  1.0000,
         1.0000,  1.0000,  0.5000,  2.0000,  0.5000,  2.0000,  0.5000,  5.0000,
         4.5000,  1.5000,  1.5000,  0.5000,  2.0000,  1.5000,  2.0000,  2.5000,
         1.5000,  1.0000,  0.5000,  0.5000,  1.0000,  4.0000,  1.5000,  1.5000,
         0.5000,  0.5000,  5.0000,  0.5000,  1.0000,  1.0000,  0.5000,  2.0000,
         0.5000,  2.5000,  1.0000,  0.5000,  1.5000,  2.0000,  1.5000,  2.5000,
         1.0000,  1.0000,  1.5000,  0.5000,  0.5000,  1.0000,  1.5000,  2.0000,
        52.0000,  4.0000,  6.0000,  2.0000,  1.5000,  1.0000,  1.5000,  1.0000,
         2.0000,  1.5000,  6.5000,  1.5000,  1.0000,  0.5000,  0.5000,  0.5000,
         1.0000,  2.5000,  1.5000,  1.0000,  1.0000,  0.5000,  1.0000,  2.0000,
         2.5000,  1.0000,  4.5000,  2.0000, 10.0000,  1.5000,  1.5000,  1.0000,
         1.0000,  1.5000,  2.5000,  0.5000,  0.5000,  1.0000,  1.5000,  3.5000,
         0.5000,  1.0000,  0.5000,  1.5000,  1.0000,  0.5000,  3.0000,  0.5000,
         1.0000,  0.5000,  0.5000,  0.5000,  4.0000,  0.5000,  1.0000,  0.5000,
         0.5000,  1.0000,  0.5000,  0.5000,  4.5000,  1.0000,  1.0000,  1.5000,
         1.0000,  0.5000,  0.5000,  0.5000,  0.5000,  1.5000,  2.0000,  5.5000,
         0.5000,  1.5000,  0.5000,  4.5000,  1.0000,  2.5000,  1.5000,  0.5000,
         0.5000,  0.5000,  0.5000,  1.0000,  3.5000,  1.0000,  0.5000,  0.5000,
         2.0000,  1.0000,  4.0000,  2.0000,  2.0000,  0.5000,  1.5000,  2.0000,
         0.5000,  1.5000,  0.5000,  0.5000,  2.0000,  0.5000,  1.0000,  4.0000,
         0.5000,  1.0000,  1.5000,  2.0000,  1.0000,  1.5000,  0.5000,  2.0000,
         1.5000,  0.5000,  1.0000,  3.5000,  1.0000,  3.5000,  0.5000,  1.5000,
         0.5000,  1.5000,  1.0000,  0.5000,  3.0000,  0.5000,  0.5000])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we define the <em>phase</em> matrix to capture the directional information so cruelly discarded by <code>A_symmetrized</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="mf">0.25</span>
<span class="n">Phase_matrix</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">q</span><span class="o">*</span><span class="p">(</span><span class="n">A_directed</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">A_directed</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Diversion-into-Torch's-Complex-Numbers">Diversion into Torch's Complex Numbers<a class="anchor-link" href="#Diversion-into-Torch's-Complex-Numbers"> </a></h2><p>Torch tensors have (beta) support for complex types, using the datatype <code>cfloat</code>. These can be instantiated with the function <code>torch.complex</code>, and, fortunately, the datatypes carry over intuitively: multiplication of real and complex values preserves the complex values.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">j</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">j</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.+0.9586j, 0.+0.1400j, 0.+0.8984j],
        [0.+0.7221j, 0.+0.7698j, 0.+0.2241j],
        [0.+0.1167j, 0.+0.7521j, 0.+0.5159j]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Unfortunately, Pytorch doesn't currently support scalar multiplication between a sparse matrix and a complex number. We have to do it the long way. Notably, we have to convert the Phase_matrix to have dtype of complex float, otherwise strange errors occur.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Phase_matrix</span> <span class="o">=</span> <span class="n">Phase_matrix</span><span class="o">.</span><span class="n">cfloat</span><span class="p">()</span>
<span class="n">imaginary_phase_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">smm</span><span class="p">(</span><span class="n">Phase_matrix</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Phase_matrix</span><span class="p">)),</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Phase_matrix</span><span class="p">)))))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Phase_matrix</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(indices=tensor([[  0,   0,   1,  ..., 180,  29, 182],
                       [ 58, 121,  80,  ..., 180, 182,  29]]),
       values=tensor([ 1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j, -1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j, -1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j,  1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  1.5708+0.j,  0.0000+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                       1.5708+0.j,  1.5708+0.j,  1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,
                      -1.5708+0.j, -1.5708+0.j, -1.5708+0.j,  1.5708+0.j]),
       size=(183, 183), nnz=649, layout=torch.sparse_coo)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similarly, the exponential is not defined element-wise for sparse matrices, but we can do point-wise multiplication via the values.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># v_phase = imaginary_phase_matrix.coalesce().values()</span>
<span class="c1"># v_phase = torch.exp(v_phase)</span>
<span class="c1"># Phase_matrix_expd = torch.sparse_coo_tensor(Phase_matrix.indices(),v_phase)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Directed-Diffusion-Matrix">Directed Diffusion Matrix<a class="anchor-link" href="#Directed-Diffusion-Matrix"> </a></h1><ul>
<li>[ ] Add support for edge weights</li>
</ul>
<p>Util to take a directed adjacency matrix, and normalize it to a diffusion matrix.</p>
<p>It accepts either sparse adjacency matrices, or PyG graphs, assuming torch's COO sparse format. Indeed, given a non-sparse adjacency matrix, it converts the matrix to a COO sparse matrix before continuing with the normalization</p>
<p>{% raw %}
$$ P = D^{-1} A $$
{% endraw %}</p>
<p>The only difficulty lays in normalization: directed graphs may have rows with zero sum. To counteract that, we set any zero rowsums equal to one, noting that they are multiplied by zero anyway, so the value (as long as not infinite) doesn't matter.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="diffusion_matrix_from_graph" class="doc_header"><code>diffusion_matrix_from_graph</code><a href="https://github.com/professorwug/directed_graphs/tree/master/directed_graphs/utils.py#L158" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>diffusion_matrix_from_graph</code>(<strong><code>A</code></strong>=<em><code>None</code></em>, <strong><code>G</code></strong>=<em><code>None</code></em>, <strong><code>self_loops</code></strong>=<em><code>5</code></em>)</p>
</blockquote>
<p>Given directed adjacency matrix (sparse or unsparse), returns sparse diffusion matrix.
Accepts tensor inputs of <code>A</code>, in COO sparse form, or dense, or can work directly from a PyG graph, given via argument <code>G</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll ensure this works with a few tests.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">diffusion_matrix_from_graph</span><span class="p">(</span><span class="n">A</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
<span class="c1"># the maximum rowsum should be one, and the min should be either one or zero</span>
<span class="n">max_row_sum</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">max_row_sum</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">max_row_sum</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor(1.0000)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="n">diffusion_matrix_from_graph</span><span class="p">(</span><span class="n">G</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
<span class="n">max_row_sum</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">max_row_sum</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">max_row_sum</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor(1.0000)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([10, 10])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Powering-the-Diffusion-Matrix">Powering the Diffusion Matrix<a class="anchor-link" href="#Powering-the-Diffusion-Matrix"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>nbdev_build_lib
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Converted 00_core.ipynb.
Converted 01_Diffusion Curvature of Directed Graphs.ipynb.
Converted 02_Directed_graph_utils.ipynb.
Converted 03_Toy_Graph_Datasets.ipynb.
Converted 12_differentiable_diffusion_curvature.ipynb.
Converted 21_Communities_Datasets.ipynb.
Converted index.ipynb.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>nbdev_build_docs
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>converting: /Users/adjourner/Projects/directed_graphs/02_Directed_graph_utils.ipynb
converting /Users/adjourner/Projects/directed_graphs/index.ipynb to README.md
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>


