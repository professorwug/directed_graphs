# AUTOGENERATED! DO NOT EDIT! File to edit: 05a01 Flow_Embedding_with_Implicit_Vectors.ipynb (unless otherwise specified).

__all__ = ['GeneralFlowEmbedder']

# Cell
import torch
from torch import nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from tqdm import trange
import warnings
import inspect

class GeneralFlowEmbedder(nn.Module):
  def __init__(self, graph, dimensions=2, l_laplacian=0.1):
    """
    Instantiate a General Flow Embedder

    Input:
      graph :
        a torch geometric graph object
      dimensions :
        the size of the embedding
      emb_init :
        a function that takes the graph and the dimensions and returns a initialized positions for the nodes
      gt_cost_fn :
        a function that takes the graph and returns pairwise (asymmetric) ground-truth cost among nodes
      emb_cost_fn :
        a function that takes the embedding and the flow and returns pairwise (asymmetric) cost among embedded nodes
      flow_gen :
        a function that takes the graph, the embedding, and the dimensions and returns a flow vector for each node
      laplacian_gen :
        a function that takes the graph and the embedding and returns a Laplacian matrix
      l_laplacian :
        a scalar to the lambda of the Laplacian regularization term

    ---
    Important dims:
      graph: n nodes, <= n^2 edges
      dimensions: d < n
      gt_cost: n x n, asymmetrical
      emb_cost: n x n, asymmetrical
      laplacian: n x n
      emb: n x d
      flow: n x d
    """
    super(GeneralFlowEmbedder, self).__init__()
    self.graph = graph
    self.num_nodes = graph.num_nodes
    self.dimensions = dimensions

    self.l_laplacian = l_laplacian

    def initialize(self):
      self.emb = nn.Parameter(self.emb_init())
      self.flow = self.flow_gen()
      self.gt_cost = self.gt_cost_fn()

    def emb_init(self):
      raise NotImplementedError(f"You need to implement {inspect.stack()[0][3]} when you inherit from Model")

    def gt_cost_fn(self):
      raise NotImplementedError(f"You need to implement {inspect.stack()[0][3]} when you inherit from Model")

    def emb_cost_fn(self):
      raise NotImplementedError(f"You need to implement {inspect.stack()[0][3]} when you inherit from Model")

    def flow_gen(self):
      raise NotImplementedError(f"You need to implement {inspect.stack()[0][3]} when you inherit from Model")

    def dense_adj_gen(self):
      raise NotImplementedError(f"You need to implement {inspect.stack()[0][3]} when you inherit from Model")

    def dense_adj_to_laplacian(self, dense_adj):
      deg = torch.diag(dense_adj.sum(axis=0))
      return deg - dense_adj

    def laplacian_gen(self):
      dense_adj = self.dense_adj_gen()
      return self.dense_adj_to_laplacian(dense_adj)

    def loss(self):
      self.emb_cost = self.emb_cost_fn(self.emb, self.flow)
      self.laplacian = self.laplacian_gen(self.graph, self.emb)
      loss = self.cost_loss(self.gt_cost, self.emb_cost) + self.l_laplacian * self.laplacian_loss(self.laplacian, self.flow)
      return loss

    def cost_loss(self, gt_cost, emb_cost):
      return torch.linalg.norm(torch.log(1 + self.gt_cost) - torch.log(1 + self.emb_cost))**2

    def laplacian_loss(self, laplacian, flow):
      loss = 0
      for i in range(self.dimensions):
        f = flow[:,i].reshape(self.num_nodes, 1)
        loss += (f.t() @ laplacian @ f)/(f.t() @ f)[0,0]
      return loss

    def fit(self, epochs=1000, restart=False):
      if restart:
        self.initialize()
      self.train()
      optim = torch.optim.Adam(self.parameters())
      for _ in trange(epochs):
        optim.zero_grad()
        loss = self.loss()
        loss.backward()
        optim.step()
      print("Exiting training with loss ", loss)
      return self.emb, self.flow
